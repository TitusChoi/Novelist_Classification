{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Novelist_Classification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TitusChoi/Novelist_Classification/blob/master/Novelist_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bv6-s_MRbZx"
      },
      "source": [
        "# Novelist_Classification.ipynb\n",
        "Description   : 소설 작가 분류에 대한 예측 모델 구현<br>\n",
        "Date : 2021.04.23 ~ 2021.04.29<br>\n",
        "Author : 최유리, 정석영, 박민춘, 최디도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwe8SeohTHVs"
      },
      "source": [
        "# 1. 필요 라이브러리 정의 및 데이터 목록 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJZJ7Yr-FUOn",
        "outputId": "04b7c797-f400-4f12-d16f-46ef7bff6b67"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv_Wml_wFeJD"
      },
      "source": [
        "import pandas as pd\n",
        "import warnings \n",
        "warnings.filterwarnings(action='ignore')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoX1-VGQFfg1"
      },
      "source": [
        "#경로 재설정\n",
        "import os\n",
        "os.chdir('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEZtMdB5CaDG"
      },
      "source": [
        "#파일 불러오기\n",
        "train = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/train.csv', encoding = 'utf-8')\n",
        "train_new = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/new_train.csv', encoding = 'utf-8')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/test_x.csv', encoding = 'utf-8')\n",
        "test_new = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/new_test.csv', encoding = 'utf-8')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/sample_submission.csv', encoding = 'utf-8')\n",
        "sample_submission_new = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/new_sample_submission.csv', encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "SvX_U2ETC7vA",
        "outputId": "1159ee1d-ef61-414b-d75b-8c1a5ab903cc"
      },
      "source": [
        "#train 데이터 살펴보기\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>He was almost choking. There was so much, so m...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>“Your sister asked for it, I suppose?”</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>She was engaged one day as she walked, in per...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The captain was in the porch, keeping himself ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54874</th>\n",
              "      <td>54874</td>\n",
              "      <td>“Is that you, Mr. Smith?” odin whispered. “I h...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54875</th>\n",
              "      <td>54875</td>\n",
              "      <td>I told my plan to the captain, and between us ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54876</th>\n",
              "      <td>54876</td>\n",
              "      <td>\"Your sincere well-wisher, friend, and sister...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54877</th>\n",
              "      <td>54877</td>\n",
              "      <td>“Then you wanted me to lend you money?”</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54878</th>\n",
              "      <td>54878</td>\n",
              "      <td>It certainly had not occurred to me before, bu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>54879 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index                                               text  author\n",
              "0          0  He was almost choking. There was so much, so m...       3\n",
              "1          1             “Your sister asked for it, I suppose?”       2\n",
              "2          2   She was engaged one day as she walked, in per...       1\n",
              "3          3  The captain was in the porch, keeping himself ...       4\n",
              "4          4  “Have mercy, gentlemen!” odin flung up his han...       3\n",
              "...      ...                                                ...     ...\n",
              "54874  54874  “Is that you, Mr. Smith?” odin whispered. “I h...       2\n",
              "54875  54875  I told my plan to the captain, and between us ...       4\n",
              "54876  54876   \"Your sincere well-wisher, friend, and sister...       1\n",
              "54877  54877            “Then you wanted me to lend you money?”       3\n",
              "54878  54878  It certainly had not occurred to me before, bu...       0\n",
              "\n",
              "[54879 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "avAxy9OeJEUf",
        "outputId": "1e79b7e0-b4ee-442c-c977-7dbc017e4fd5"
      },
      "source": [
        "train_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>It is hard to forget repulsive things. I remem...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It would be tedious if given in the beadle’s w...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>“Very good. Shall we argue about it here in p...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>“What! and I as high as a tree and as big as a...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"Isn't it enough, Vanya?\" she cried, seeing hi...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83215</th>\n",
              "      <td>83215</td>\n",
              "      <td>What with the birthday visitors, and what with...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83216</th>\n",
              "      <td>83216</td>\n",
              "      <td>It was an old rickety door and gave at once be...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83217</th>\n",
              "      <td>83217</td>\n",
              "      <td>“Well then you can go to the devil,” said odin...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83218</th>\n",
              "      <td>83218</td>\n",
              "      <td>“Don’t know?”</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83219</th>\n",
              "      <td>83219</td>\n",
              "      <td>“Not go to town!” cried Mrs. Palmer, with a la...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>83220 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                                               text  author\n",
              "0               0  It is hard to forget repulsive things. I remem...       6\n",
              "1               1  It would be tedious if given in the beadle’s w...       7\n",
              "2               2   “Very good. Shall we argue about it here in p...       2\n",
              "3               3  “What! and I as high as a tree and as big as a...       6\n",
              "4               4  \"Isn't it enough, Vanya?\" she cried, seeing hi...       3\n",
              "...           ...                                                ...     ...\n",
              "83215       83215  What with the birthday visitors, and what with...       7\n",
              "83216       83216  It was an old rickety door and gave at once be...       2\n",
              "83217       83217  “Well then you can go to the devil,” said odin...       3\n",
              "83218       83218                                      “Don’t know?”       7\n",
              "83219       83219  “Not go to town!” cried Mrs. Palmer, with a la...       5\n",
              "\n",
              "[83220 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "8MxZsr6yCshr",
        "outputId": "cf956466-1a77-4bc1-8d07-541c4a9f72eb"
      },
      "source": [
        "#test 데이터 살펴보기\n",
        "del test_new['Unnamed: 0']\n",
        "test_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>About thirty years ago Miss Maria Ward, of Hun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Their homes were so distant, and the circles i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>The letter was not unproductive. It re-establi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Such were its immediate effects, and within a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Sir Thomas could not give so instantaneous and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27313</th>\n",
              "      <td>27313</td>\n",
              "      <td>At the end of another day or two, odin growing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27314</th>\n",
              "      <td>27314</td>\n",
              "      <td>All afternoon we sat together, mostly in silen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27315</th>\n",
              "      <td>27315</td>\n",
              "      <td>odin, having carried his thanks to odin, proc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27316</th>\n",
              "      <td>27316</td>\n",
              "      <td>Soon after this, upon odin's leaving the room,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27317</th>\n",
              "      <td>27317</td>\n",
              "      <td>And all the worse for the doomed man, that the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27318 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index                                               text\n",
              "0          0  About thirty years ago Miss Maria Ward, of Hun...\n",
              "1          1  Their homes were so distant, and the circles i...\n",
              "2          2  The letter was not unproductive. It re-establi...\n",
              "3          3  Such were its immediate effects, and within a ...\n",
              "4          4  Sir Thomas could not give so instantaneous and...\n",
              "...      ...                                                ...\n",
              "27313  27313  At the end of another day or two, odin growing...\n",
              "27314  27314  All afternoon we sat together, mostly in silen...\n",
              "27315  27315   odin, having carried his thanks to odin, proc...\n",
              "27316  27316  Soon after this, upon odin's leaving the room,...\n",
              "27317  27317  And all the worse for the doomed man, that the...\n",
              "\n",
              "[27318 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "WOpmyyX_77Op",
        "outputId": "55fd3966-aaf2-4706-bdf0-55ebe76b883c"
      },
      "source": [
        "#sample_submission\n",
        "del sample_submission_new['Unnamed: 0']\n",
        "sample_submission_new = sample_submission_new.drop(sample_submission_new.index[27318])\n",
        "sample_submission_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27313</th>\n",
              "      <td>27313</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27314</th>\n",
              "      <td>27314</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27315</th>\n",
              "      <td>27315</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27316</th>\n",
              "      <td>27316</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27317</th>\n",
              "      <td>27317</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27318 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index  0  1  2  3  4  5  6  7\n",
              "0          0  0  0  0  0  0  0  0  0\n",
              "1          1  0  0  0  0  0  0  0  0\n",
              "2          2  0  0  0  0  0  0  0  0\n",
              "3          3  0  0  0  0  0  0  0  0\n",
              "4          4  0  0  0  0  0  0  0  0\n",
              "...      ... .. .. .. .. .. .. .. ..\n",
              "27313  27313  0  0  0  0  0  0  0  0\n",
              "27314  27314  0  0  0  0  0  0  0  0\n",
              "27315  27315  0  0  0  0  0  0  0  0\n",
              "27316  27316  0  0  0  0  0  0  0  0\n",
              "27317  27317  0  0  0  0  0  0  0  0\n",
              "\n",
              "[27318 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPq0vxJVDS3R"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52ZAJAyaUJgp"
      },
      "source": [
        "## 2.1. 문장 부호를 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMUMcInsD80p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "c501123e-df85-483e-e927-1227643a387b"
      },
      "source": [
        "def alpha_num(text):\n",
        "    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n",
        "\n",
        "train['text']=train['text'].apply(alpha_num)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-aded63c2d4f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^A-Za-z0-9 ]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsv8RK2VUNJp"
      },
      "source": [
        "## 2.2. 불용어 처리전 데이터 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf_TGrbKCaDK"
      },
      "source": [
        "# 불용어 제거해주는 함수\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in base_stopwords:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "# 불용어\n",
        "base_stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
        "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
        "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
        "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
        "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
        "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
        "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
        "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
        "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
        "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
        "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUgcc07ADiiU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b82bd22a-bd2b-4a27-f9f6-d39941d3d4f4"
      },
      "source": [
        "#전처리 직접 적용\n",
        "train['text'] = train['text'].str.lower()\n",
        "test['text'] = test['text'].str.lower()\n",
        "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords)\n",
        "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)\n",
        "\n",
        "\n",
        "train_new['text'] = train_new['text'].str.lower()\n",
        "test_new['text'] = test_new['text'].str.lower()\n",
        "train_new['text'] = train_new['text'].apply(alpha_num).apply(remove_stopwords)\n",
        "test_new['text'] = test_new['text'].apply(alpha_num).apply(remove_stopwords)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8caafdb21be2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#전처리 직접 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZdWzRkCDovd"
      },
      "source": [
        "# train test 분리\n",
        "X_train = np.array([x for x in train['text']])\n",
        "X_test = np.array([x for x in test['text']])\n",
        "y_train = np.array([x for x in train['author']])\n",
        "\n",
        "X_train_new = np.array([x for x in train_new['text']])\n",
        "X_test_new = np.array([x for x in test_new['text']])\n",
        "y_train_new = np.array([x for x in train_new['author']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz4FgCyNZnC0"
      },
      "source": [
        "## 2.3. nltk 라이브러리 활용 불용어 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8t_KqwMZreO"
      },
      "source": [
        "# nltk 라이브러리를 사용한 불용어 제거\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk_stopword = set(stopwords.words('english'))\n",
        "\n",
        "# Stopword 만 제거한 결과\n",
        "def remove_stopwords_nltk(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in nltk_stopword:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdLc4_4JZu-y"
      },
      "source": [
        "# 단어 길이 확인\n",
        "tmp = train\n",
        "\n",
        "for i in range(len(tmp['text'])):\n",
        "    tmp['text'][i] = tmp['text'][i].strip().split()\n",
        "\n",
        "vocab = nltk.FreqDist(np.hstack(tmp['text']))\n",
        "print('단어 집합의 크기 : {}'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b71ontTUavF"
      },
      "source": [
        "## 2.4. 작가별 텍스트 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ue4lDCHT0nR"
      },
      "source": [
        "# 작가별 text 나누는 함수 정의\n",
        "author_0 = train.loc[train['author'] == 0, 'text']\n",
        "author_1 = train.loc[train['author'] == 1, 'text']\n",
        "author_2 = train.loc[train['author'] == 2, 'text']\n",
        "author_3 = train.loc[train['author'] == 3, 'text']\n",
        "author_4 = train.loc[train['author'] == 4, 'text']\n",
        "author_5 = train.loc[train['author'] == 5, 'text']\n",
        "author_6 = train.loc[train['author'] == 6, 'text']\n",
        "author_7 = train.loc[train['author'] == 7, 'text']\n",
        "\n",
        "print(author_0.shape)\n",
        "print(author_1.shape)\n",
        "print(author_2.shape)\n",
        "print(author_3.shape)\n",
        "print(author_4.shape)\n",
        "print(author_5.shape)\n",
        "print(author_6.shape)\n",
        "print(author_7.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-RaCGV1T3t7"
      },
      "source": [
        "# 작가별 text길이 시각화\n",
        "sns.set_palette(\"Paired\")\n",
        "fig, axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(12,5)\n",
        "sns.countplot(y_train,)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1E1YIbJT58T"
      },
      "source": [
        "# keras 문장 토큰화\n",
        "## text_to_word_sequence는 알파벳을 소문자로 바꾸고, Punctuation을 제거한다\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "\n",
        "# 시리즈를 리스트로, 리스트를 문자열로 바꾸는 함수 (text_to_word_sequence가 문자열만 입력값으로 받기 때문)\n",
        "def series_list_to_str(a):\n",
        "    a.to_list()\n",
        "    return \" \".join([str(_) for _ in a])\n",
        "\n",
        "\n",
        "str_author_0 = series_list_to_str(author_0)\n",
        "str_author_1 = series_list_to_str(author_1)\n",
        "str_author_2 = series_list_to_str(author_2)\n",
        "str_author_3 = series_list_to_str(author_3)\n",
        "str_author_4 = series_list_to_str(author_4)\n",
        "str_author_5 = series_list_to_str(author_5)\n",
        "str_author_6 = series_list_to_str(author_6)\n",
        "str_author_7 = series_list_to_str(author_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxpFY8EOT6yA"
      },
      "source": [
        "# 문장을 단어로 바꿔주는 함수\n",
        "author_word_0 = text_to_word_sequence(str_author_0)\n",
        "author_word_1 = text_to_word_sequence(str_author_1)\n",
        "author_word_2 = text_to_word_sequence(str_author_2)\n",
        "author_word_3 = text_to_word_sequence(str_author_3)\n",
        "author_word_4 = text_to_word_sequence(str_author_4)\n",
        "author_word_5 = text_to_word_sequence(str_author_5)\n",
        "author_word_6 = text_to_word_sequence(str_author_6)\n",
        "author_word_7 = text_to_word_sequence(str_author_7)\n",
        "print(author_word_0[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyaIMdxnT8U_"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j33qQiIAT92I"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "# fit_on_texts 적용 함수\n",
        "# fit_on_texts()는 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여\n",
        "\n",
        "\n",
        "def fitontexts(words):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts([words])\n",
        "    return tokenizer.word_index\n",
        "\n",
        "\n",
        "# 이 과정으로 작가별 빈도수가 높은 단어를 구할 수 있음.\n",
        "author0_index = fitontexts(author_word_0)\n",
        "author1_index = fitontexts(author_word_1)\n",
        "author2_index = fitontexts(author_word_2)\n",
        "author3_index = fitontexts(author_word_3)\n",
        "author4_index = fitontexts(author_word_4)\n",
        "author5_index = fitontexts(author_word_5)\n",
        "author6_index = fitontexts(author_word_6)\n",
        "author7_index = fitontexts(author_word_7)\n",
        "\n",
        "\n",
        "\n",
        "# print(author0_index)\n",
        "# print(author1_index)\n",
        "# print(author2_index)\n",
        "# print(author3_index)\n",
        "# print(author4_index)\n",
        "# print(author5_index)\n",
        "# print(author6_index)\n",
        "# print(author7_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAMTNk45UAMc"
      },
      "source": [
        "# 상위 30개 많이 쓰인 단어\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "fig = plt.figure(figsize=(17, 8))\n",
        "rows = 2\n",
        "cols = 4\n",
        "\n",
        "\n",
        "def plot_top_non_stopwords_barchart(text):\n",
        "\n",
        "    new = text.str.split()\n",
        "    new = new.values.tolist()\n",
        "    corpus = [word for i in new for word in i]\n",
        "\n",
        "    counter = Counter(corpus)\n",
        "    most = counter.most_common()\n",
        "    x, y = [], []\n",
        "    for word, count in most[:30]:\n",
        "        x.append(word)\n",
        "        y.append(count)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    x, y = plot_top_non_stopwords_barchart(\n",
        "        train[train['author'] == i]['text'])\n",
        "    ax = fig.add_subplot(rows, cols, i + 1)\n",
        "    ax.set_title(i)\n",
        "    sns.barplot(x=y, y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD1ZpKGJUDWs"
      },
      "source": [
        "# 작가별 많이 쓴 단어 상위 10개\n",
        "fig = plt.figure(figsize=(17, 8))  # rows*cols 행렬의 i번째 subplot 생성\n",
        "rows = 2\n",
        "cols = 4\n",
        "\n",
        "\n",
        "def plot_top_non_stopwords_barchart2(text):\n",
        "\n",
        "    new = text.str.split()\n",
        "    new = new.values.tolist()\n",
        "    corpus = [word for i in new for word in i]\n",
        "\n",
        "    counter = Counter(corpus)\n",
        "    most = counter.most_common()\n",
        "    x, y = [], []\n",
        "    for word, count in most[:10]:\n",
        "        x.append(word)\n",
        "        y.append(count)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    x, y = plot_top_non_stopwords_barchart2(\n",
        "        train[train['author'] == i]['text'])\n",
        "    ax = fig.add_subplot(rows, cols, i + 1)\n",
        "    ax.set_title(i)\n",
        "    sns.barplot(x=y, y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJz_J1v9Uu82"
      },
      "source": [
        "### 2.4.1. N-gram 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoqPuJjFUESJ"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import  Counter\n",
        "\n",
        "def plot_top_ngrams_barchart(text, n=2):\n",
        "    stop=set(base_stopwords)\n",
        "\n",
        "    new= text.str.split()\n",
        "    new=new.values.tolist()\n",
        "    corpus=[word for i in new for word in i]\n",
        "\n",
        "    def _get_top_ngram(corpus, n=None):\n",
        "        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
        "        bag_of_words = vec.transform(corpus)\n",
        "        sum_words = bag_of_words.sum(axis=0) \n",
        "        words_freq = [(word, sum_words[0, idx]) \n",
        "                      for word, idx in vec.vocabulary_.items()]\n",
        "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "        return words_freq[:10]\n",
        "\n",
        "    top_n_bigrams=_get_top_ngram(text,n)[:10]\n",
        "    x,y=map(list,zip(*top_n_bigrams))\n",
        "    sns.barplot(x=y,y=x)\n",
        "\n",
        "plot_top_ngrams_barchart(train['text'], n=3)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O07bpzRxU3Kz"
      },
      "source": [
        "fig = plt.figure(figsize=(16,10)) # rows*cols 행렬의 i번째 subplot 생성\n",
        "rows = 2\n",
        "cols = 4\n",
        "\n",
        "def plot_top_ngrams_barchart2(text, n=2):\n",
        "    stop=set(base_stopwords)\n",
        "\n",
        "    new= text.str.split()\n",
        "    new=new.values.tolist()\n",
        "    corpus=[word for i in new for word in i]\n",
        "\n",
        "    def _get_top_ngram(corpus, n=None):\n",
        "        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
        "        bag_of_words = vec.transform(corpus)\n",
        "        sum_words = bag_of_words.sum(axis=0) \n",
        "        words_freq = [(word, sum_words[0, idx]) \n",
        "                      for word, idx in vec.vocabulary_.items()]\n",
        "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "        return words_freq[:10]\n",
        "\n",
        "    top_n_bigrams=_get_top_ngram(text,n)[:10]\n",
        "    x,y=map(list,zip(*top_n_bigrams))\n",
        "    \n",
        "    return x, y\n",
        "\n",
        "for i in range(8):\n",
        "    x,y = plot_top_ngrams_barchart2(train[train['author']==i]['text'], n=3)\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    ax.set_title(i)\n",
        "    sns.barplot(x=y,y=x)\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVLLGv9lU6sM"
      },
      "source": [
        "### 2.4.2. 토픽별 EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQuTs7ZGVn5z"
      },
      "source": [
        "!pip install pyLDAvis=='2.1.2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc_hjL-gVp9f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAok1VAOVsm5"
      },
      "source": [
        "# With NLTK you can tokenize and lemmatize easily:\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "NUM_TOPICS = 8\n",
        "\n",
        "\n",
        "def get_lda_objects(text):\n",
        "    nltk.download('stopwords')\n",
        "    stop = set(base_stopwords)\n",
        "\n",
        "    def _preprocess_text(text):\n",
        "        corpus = []\n",
        "        stem = PorterStemmer()\n",
        "        lem = WordNetLemmatizer()\n",
        "        for news in text:\n",
        "            words = [w for w in word_tokenize(news) if (w not in stop)]\n",
        "\n",
        "            words = [lem.lemmatize(w) for w in words if len(w) > 2]\n",
        "\n",
        "            corpus.append(words)\n",
        "        return corpus\n",
        "\n",
        "    corpus = _preprocess_text(text)\n",
        "\n",
        "    dic = gensim.corpora.Dictionary(corpus)\n",
        "    bow_corpus = [dic.doc2bow(doc) for doc in corpus]\n",
        "\n",
        "    lda_model = gensim.models.LdaMulticore(bow_corpus,\n",
        "                                           num_topics=NUM_TOPICS,\n",
        "                                           id2word=dic,\n",
        "                                           passes=10,\n",
        "                                           workers=2)\n",
        "\n",
        "    return lda_model, bow_corpus, dic\n",
        "\n",
        "\n",
        "def plot_lda_vis(lda_model, bow_corpus, dic):\n",
        "    pyLDAvis.enable_notebook()\n",
        "    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dic)\n",
        "    return vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgw1bbb6VvPa"
      },
      "source": [
        "lda_model, bow_corpus, dic = get_lda_objects(train['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8_Gb8ylVxEA"
      },
      "source": [
        "lda_model.show_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKVYdUDJVyNE"
      },
      "source": [
        "plot_lda_vis(lda_model, bow_corpus, dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRfpgAniV2qT"
      },
      "source": [
        "### 2.4.3. 훈련 데이터 LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpf3Ssr_V761"
      },
      "source": [
        "lda_model, bow_corpus, dic = get_lda_objects(test['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkcq5lpjV8X4"
      },
      "source": [
        "plot_lda_vis(lda_model, bow_corpus, dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AGQ9XuiWA-6"
      },
      "source": [
        "### 2.4.4. 감정 분석(Sentiment Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgmSh6ShWL4o"
      },
      "source": [
        "from textblob import TextBlob\n",
        "    \n",
        "def plot_polarity_histogram(text):\n",
        "    \n",
        "    def _polarity(text):\n",
        "        return TextBlob(text).sentiment.polarity\n",
        "        \n",
        "    polarity_score = text.apply(lambda x : _polarity(x))\n",
        "    return polarity_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLVF-ux7WNHD"
      },
      "source": [
        "fig = plt.figure(figsize=(10,5)) # rows*cols 행렬의 i번째 subplot 생성\n",
        "rows = 2\n",
        "cols = 4\n",
        "\n",
        "for i in range(8):\n",
        "    polarity_score = plot_polarity_histogram(train[train['author']==i]['text'])\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    ax.set_title(i)\n",
        "    polarity_score.hist()\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeiIZK1FWT0Y"
      },
      "source": [
        "### 2.4.5. 평가 자료 극성 분석(Test Data Polarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNi4wjNFWawj"
      },
      "source": [
        "polarity_score = plot_polarity_histogram(test['text'])\n",
        "polarity_score.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bca0FO3rWgGY"
      },
      "source": [
        "def sentiment_vader(text, sid):\n",
        "    ss = sid.polarity_scores(text)\n",
        "    ss.pop('compound')\n",
        "    return max(ss, key=ss.get)\n",
        "\n",
        "def sentiment_textblob(text):\n",
        "        x = TextBlob(text).sentiment.polarity\n",
        "        \n",
        "        if x<0:\n",
        "            return 'neg'\n",
        "        elif x==0:\n",
        "            return 'neu'\n",
        "        else:\n",
        "            return 'pos'\n",
        "\n",
        "def plot_sentiment_barchart(text, method='TextBlob'):\n",
        "    if method == 'TextBlob':\n",
        "        sentiment = text.map(lambda x: sentiment_textblob(x))\n",
        "    elif method == 'Vader':\n",
        "        nltk.download('vader_lexicon')\n",
        "        sid = SentimentIntensityAnalyzer()\n",
        "        sentiment = text.map(lambda x: sentiment_vader(x, sid=sid))\n",
        "    else:\n",
        "        raise ValueError('Textblob or Vader')\n",
        "    \n",
        "    return sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WnXIjxuWihI"
      },
      "source": [
        "fig = plt.figure(figsize=(10,5)) # rows*cols 행렬의 i번째 subplot 생성\n",
        "rows = 2\n",
        "cols = 4\n",
        "\n",
        "for i in range(8):\n",
        "    sentiment = plot_sentiment_barchart(train[train['author']==i]['text'])\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    #plt.bar(sentiment.value_counts().index, sentiment.value_counts())\n",
        "    ax.set_title(i)\n",
        "    sns.barplot(sentiment.value_counts().index, sentiment.value_counts())\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32zQe4PlWn5t"
      },
      "source": [
        "### 2.4.6. 평가 데이터 감정 분석(Test Data Sentiment Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoyY2YTtWudL"
      },
      "source": [
        "sentiment = plot_sentiment_barchart(test['text'])\n",
        "sns.barplot(sentiment.value_counts().index, sentiment.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRlMVQ-9W_6w"
      },
      "source": [
        "### 2.4.7 개체명인식(NER : Named Entity Recognition)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0NYDqkWW_Jj"
      },
      "source": [
        "import spacy\n",
        "from collections import  Counter\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_named_entity_barchart(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "    def _get_ner(text):\n",
        "        doc=nlp(text)\n",
        "        return [X.label_ for X in doc.ents]\n",
        "    \n",
        "    ent=text.apply(lambda x : _get_ner(x))\n",
        "    ent=[x for sub in ent for x in sub]\n",
        "    counter=Counter(ent)\n",
        "    count=counter.most_common()\n",
        "    \n",
        "    x,y=map(list,zip(*count))\n",
        "    return x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15rzKo45XV8D"
      },
      "source": [
        "fig = plt.figure(figsize=(20,20)) # rows*cols 행렬의 i번째 subplot 생성\n",
        "rows = 2\n",
        "cols = 4\n",
        "\n",
        "for i in range(8):\n",
        "    x, y = plot_named_entity_barchart(train[train['author']==i]['text'])\n",
        "    ax = fig.add_subplot(rows, cols, i+1)\n",
        "    ax.set_title(i)\n",
        "    sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OaD3QknXX60"
      },
      "source": [
        "### 2.4.8 사람 이름 분포"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNQ3EDjqXfJg"
      },
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def plot_most_common_named_entity_barchart(text, entity=\"PERSON\"):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def _get_ner(text, ent):\n",
        "        doc = nlp(text)\n",
        "        return [X.text for X in doc.ents if X.label_ == ent]\n",
        "\n",
        "    entity_filtered = text.apply(lambda x: _get_ner(x, entity))\n",
        "    entity_filtered = [i for x in entity_filtered for i in x]\n",
        "\n",
        "    counter = Counter(entity_filtered)\n",
        "    x, y = map(list, zip(*counter.most_common(10)))\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46E6FLjfXg0m"
      },
      "source": [
        "fig = plt.figure(figsize=(20, 15))  # rows*cols 행렬의 i번째 subplot 생성\n",
        "rows = 2\n",
        "cols = 4\n",
        "\n",
        "for i in range(8):\n",
        "    x, y = plot_most_common_named_entity_barchart(\n",
        "        train[train['author'] == i]['text'], entity=\"PERSON\")\n",
        "    ax = fig.add_subplot(rows, cols, i + 1)\n",
        "    sns.barplot(y, x).set_title(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLAA6GTjYHyD"
      },
      "source": [
        "# 3. 데이터 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVo_poTEZ_qh"
      },
      "source": [
        "## 3.1. 임베딩에 필요한 라이브러리 및 사전 학습 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwF2WpDSYKJ8"
      },
      "source": [
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_N3_9TRaJ22"
      },
      "source": [
        "pip install -U gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHjYxDCyaKX8"
      },
      "source": [
        "FastText = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Novelist_Classification/Embedding/fasttext.vec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kJ4-CyBaPd2"
      },
      "source": [
        "Word2Vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "    '/content/drive/MyDrive/Novelist_Classification/Embedding/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv2uxENlaRnb"
      },
      "source": [
        "#파라미터 설정\n",
        "vocab_size = 63727\n",
        "embedding_dim = 16\n",
        "max_length = 212\n",
        "padding_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrPvlEbeaUAp"
      },
      "source": [
        "#tokenizer에 fit\n",
        "tokenizer = Tokenizer(num_words = vocab_size) #, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxkMRI3kaVxr"
      },
      "source": [
        "#데이터를 sequence로 변환해주고 padding 해줍니다.\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "# len(train_sequences)\n",
        "idx = []\n",
        "for i in range(len(train_sequences)):\n",
        "    if len(train_sequences[i]) > 213 :\n",
        "        idx.append(i)\n",
        "\n",
        "# len(idx)\n",
        "# type(X_train)\n",
        "# X_train = np.delete(X_train,idx,0)\n",
        "# y_train = np.delete(y_train, idx)\n",
        "\n",
        "train = train.drop(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzfB2BnOaXYp"
      },
      "source": [
        "tmp = train\n",
        "\n",
        "vocab = nltk.FreqDist(np.hstack(tmp['text']))\n",
        "print('단어 집합의 크기 : {}'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP-Kl7axaZAE"
      },
      "source": [
        "X_train = np.array([x for x in train['text']])\n",
        "X_test = np.array([x for x in test['text']])\n",
        "y_train = np.array([x for x in train['author']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qeVJEi1awcy"
      },
      "source": [
        "#단어 집합 크기에 따른 단어파라미터 재설정\n",
        "vocab_size = 63162"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc2fjFCDaaQT"
      },
      "source": [
        "#tokenizer에 fit\n",
        "tokenizer = Tokenizer(num_words = vocab_size) #, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnXbNVO4akdO"
      },
      "source": [
        "#데이터를 sequence로 변환해주고 padding 해줍니다.\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "# train_sequences\n",
        "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "# train_padded\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJUvGNxWa5Sn"
      },
      "source": [
        "## 3.2. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSimfrAka79r"
      },
      "source": [
        "# Word2Vec 임베딩 과정\n",
        "W2V_embedding_matrix = np.zeros((vocab_size,300))\n",
        "\n",
        "def get_vector(word):\n",
        "    if word in Word2Vec_model:\n",
        "        return Word2Vec_model[word]\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_6db_74bAG1"
      },
      "source": [
        "# print(vocab.items())\n",
        "for idx, word in word_index.items():\n",
        "\n",
        "    temp = get_vector(word)\n",
        "    if temp is not None:\n",
        "        W2V_embedding_matrix[idx] = temp\n",
        "        \n",
        "print(W2V_embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HXbN_gPbBWz"
      },
      "source": [
        "vocab.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPvurIvabC13"
      },
      "source": [
        "word_index.items()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIk7X9tZbEJM"
      },
      "source": [
        "## 3.3. Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAsX9kgXbH0f"
      },
      "source": [
        "# Fasttext 임베딩 과정\n",
        "FT_embedding_matrix = np.zeros((vocab_size,100))\n",
        "\n",
        "def get_vector(word):\n",
        "    if word in FastText:\n",
        "        return FastText[word]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "for word, idx in word_index.items():\n",
        "    temp = get_vector(word)\n",
        "    if temp is not None:\n",
        "        FT_embedding_matrix[idx] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZNSfxWqbNdf"
      },
      "source": [
        "## 3.4. Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgkqfp0wbPcb"
      },
      "source": [
        "# Glove 임베딩 과정\n",
        "embedding_dict= dict()\n",
        "f = open('/content/drive/MyDrive/Novelist_Classification/Embedding/glove.txt', encoding='utf8')\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close\n",
        "\n",
        "glove_embedding_matrix = np.zeros((vocab_size+1, 50))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    temp = embedding_dict.get(word)\n",
        "    if temp is not None:\n",
        "        glove_embedding_matrix[i] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrLhpchgbggb"
      },
      "source": [
        "# 4. 모델 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPcdBH6IbnOj"
      },
      "source": [
        "## 4.1. Simple DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnm-AdYibmPk"
      },
      "source": [
        "# 간단한 DNN 모델\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAPNJrX2b-Wi"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Nni8B9cBHk"
      },
      "source": [
        "# fit model\n",
        "num_epochs = 20\n",
        "history = model.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, \n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70bX6zoTcFJN"
      },
      "source": [
        "결과에 따라 우리는 소설 작가 예측 모델을 Fasttext 임베딩과 Glove 임베딩만을 적용해보기로 했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioyYrfecdL4K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}