{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baseline_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZWbUUwqKBdie"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TitusChoi/Novelist_Classification/blob/yuls/Baseline_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufcGB_euCc7M"
      },
      "source": [
        "# **데이터 살펴보기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr1zgWURJc7L",
        "outputId": "00e50f11-d14a-4282-d1e8-e06b8228bc0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmA1Au6CCaC7"
      },
      "source": [
        "import pandas as pd\n",
        "import warnings \n",
        "warnings.filterwarnings(action='ignore')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import re\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZwTCLUOCtUu"
      },
      "source": [
        "#경로 설정\n",
        "import os\n",
        "os.chdir('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEZtMdB5CaDG"
      },
      "source": [
        "#파일 불러오기\n",
        "train = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/new_train.csv', encoding = 'utf-8')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/new_test.csv', encoding = 'utf-8')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Novelist_Classification/datasets/new_sample_submission.csv', encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvX_U2ETC7vA"
      },
      "source": [
        "#train 데이터 살펴보기\n",
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MxZsr6yCshr"
      },
      "source": [
        "#test 데이터 살펴보기\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOpmyyX_77Op"
      },
      "source": [
        "#sample_submission\n",
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hrcDzbmNHf5"
      },
      "source": [
        "#소설가 데이터 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdsf7D1ANOEH"
      },
      "source": [
        "# 새로운 소설데이터 폴더 주소\n",
        "path = '/content/drive/MyDrive/Novelist_Classification/datasets/'\n",
        "# 소설가 이름 폴더\n",
        "novelist = ['Charles Dickens/', 'Jane Austen/', 'Mark Twain/']\n",
        "# 소설가별 소설 txt파일 명\n",
        "cd_novel = [\n",
        "    'A Tale of Two Cities.txt', 'David Copperfield.txt',\n",
        "    'Great Expectations.txt', 'Oliver Twist.txt'\n",
        "]\n",
        "ja_novel = [\n",
        "    'emma.txt', 'Persuasion.txt', 'Pride and Prejudice.txt',\n",
        "    'Sense and Sensibility.txt'\n",
        "]\n",
        "mt_novel = [\n",
        "    'Adventures of Huckleberry Finn.txt', 'Life on the Mississippi.txt',\n",
        "    'The Innocents Abroad.txt', 'The Prince and the Pauper.txt'\n",
        "]\n",
        "\n",
        "# 데이터를 모을 리스트\n",
        "novel_data = list()\n",
        "\n",
        "# 기존 train.csv 데이터\n",
        "train = pd.read_csv(path + 'train.csv', encoding='utf-8')\n",
        "\n",
        "# 형식은 기존 train.csv와 같이 index,text,author 분류\n",
        "\n",
        "# train데이터에서 text와 author을 가져와 새로운 데이터 리스트에 저장\n",
        "for i in range(len(train)):\n",
        "    novel_data.append(',\"' + train['text'][i] + '\",' +\n",
        "                      str(train['author'][i]) + '\\n')\n",
        "\n",
        "# 새로운 소설 데이터 읽기 및 novel_data에 저장\n",
        "for i, name in enumerate(novelist):\n",
        "    now_novel = list()\n",
        "    if i == 0: now_novel = cd_novel\n",
        "    elif i == 1: now_novel = ja_novel\n",
        "    elif i == 2: now_novel = mt_novel\n",
        "    for novel in now_novel:\n",
        "        with open(path + name + novel, 'r') as f:\n",
        "            while True:\n",
        "                line = f.readline()\n",
        "                if not line:\n",
        "                    break\n",
        "                if len(line) < 30:\n",
        "                    continue\n",
        "                novel_data.append(',\"' + line[:-1] + '\",' + str(i + 5) + '\\n')\n",
        "\n",
        "# 모든 데이터 셔플\n",
        "random.shuffle(novel_data)\n",
        "\n",
        "# new_train.csv에 데이터 쓰기\n",
        "with open('new_train.csv', 'w') as f:\n",
        "    f.write('index,text,author\\n')\n",
        "    for i, data in enumerate(novel_data):\n",
        "        f.write(str(i) + data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPq0vxJVDS3R"
      },
      "source": [
        "# **전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMUMcInsD80p"
      },
      "source": [
        "#부호를 제거해주는 함수\n",
        "def alpha_num(text):\n",
        "    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n",
        "\n",
        "train['text']=train['text'].apply(alpha_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Zr3z_5S8FvIr",
        "outputId": "2ffcedac-cd69-4a89-9e57-5a54d1bd6c21"
      },
      "source": [
        "#부호가 사라진 것을 확인할 수 있습니다.\n",
        "train['text'][4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Have mercy gentlemen odin flung up his hands Dont write that anyway have some shame Here Ive torn my heart asunder before you and you seize the opportunity and are fingering the wounds in both halves Oh my God'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf_TGrbKCaDK"
      },
      "source": [
        "# 불용어 제거해주는 함수\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in base_stopwords:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "# 불용어\n",
        "base_stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
        "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
        "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
        "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
        "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
        "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
        "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
        "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
        "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
        "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
        "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAIa0MqOunZH",
        "outputId": "1462091d-07f3-405b-ab09-54231d3a061d"
      },
      "source": [
        "len(base_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUgcc07ADiiU"
      },
      "source": [
        "#전처리 적용\n",
        "train['text'] = train['text'].str.lower()\n",
        "test['text'] = test['text'].str.lower()\n",
        "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords)\n",
        "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZdWzRkCDovd"
      },
      "source": [
        "# train test 분리\n",
        "X_train = np.array([x for x in train['text']])\n",
        "X_test = np.array([x for x in test['text']])\n",
        "y_train = np.array([x for x in train['author']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcv2XGIIb62n",
        "outputId": "36c2ca85-6123-4aa0-8649-f11c7c496c99"
      },
      "source": [
        "# nltk 라이브러리를 사용한 불용어 제거\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk_stopword = set(stopwords.words('english'))\n",
        "\n",
        "# Stopword 만 제거한 결과\n",
        "def remove_stopwords_nltk(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in nltk_stopword:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "\n",
        "train['text'] = train['text'].str.lower()\n",
        "test['text'] = test['text'].str.lower()\n",
        "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords_nltk)\n",
        "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords_nltk)\n",
        "\n",
        "X_train = np.array([x for x in train['text']])\n",
        "X_test = np.array([x for x in test['text']])\n",
        "y_train = np.array([x for x in train['author']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rPSwQ0Zuau1",
        "outputId": "68f69fc2-0d09-492b-9466-e86fbc307ad1"
      },
      "source": [
        "len(nltk_stopword)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "_y-lmrmVSGCd",
        "outputId": "b2d89011-7484-4ccb-e6ee-859638a50555"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>almost choking much much wanted say strange ex...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>sister asked suppose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>engaged one day walked perusing janes last let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>captain porch keeping carefully way treacherou...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>mercy gentlemen odin flung hands dont write an...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54874</th>\n",
              "      <td>54874</td>\n",
              "      <td>mr smith odin whispered hardly dared hope come</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54875</th>\n",
              "      <td>54875</td>\n",
              "      <td>told plan captain us settled details accomplis...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54876</th>\n",
              "      <td>54876</td>\n",
              "      <td>sincere wellwisher friend sister lucy odin</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54877</th>\n",
              "      <td>54877</td>\n",
              "      <td>wanted lend money</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54878</th>\n",
              "      <td>54878</td>\n",
              "      <td>certainly occurred said yes like</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>54879 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index                                               text  author\n",
              "0          0  almost choking much much wanted say strange ex...       3\n",
              "1          1                               sister asked suppose       2\n",
              "2          2  engaged one day walked perusing janes last let...       1\n",
              "3          3  captain porch keeping carefully way treacherou...       4\n",
              "4          4  mercy gentlemen odin flung hands dont write an...       3\n",
              "...      ...                                                ...     ...\n",
              "54874  54874     mr smith odin whispered hardly dared hope come       2\n",
              "54875  54875  told plan captain us settled details accomplis...       4\n",
              "54876  54876         sincere wellwisher friend sister lucy odin       1\n",
              "54877  54877                                  wanted lend money       3\n",
              "54878  54878                   certainly occurred said yes like       0\n",
              "\n",
              "[54879 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks1FhSNUHrke"
      },
      "source": [
        "# **모델링**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ54B-XsJY9y"
      },
      "source": [
        "# FastText 사용\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "APPwGz_k9bf0",
        "outputId": "a63e1ee3-9a67-4a97-d5ab-1b36c1cf3451"
      },
      "source": [
        "pip install -U gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/f1417772965652d4ca6f901515debcd9d6c5430969e8c02ee7737e6de61c/gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 165kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2jbc-II80YC"
      },
      "source": [
        "FastText = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Novelist_Classification/embbeding/fasttext.vec')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbPvAJRIFoKu"
      },
      "source": [
        "Word2Vec_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Novelist_Classification/embbeding/GoogleNews-vectors-negative300.bin.gz', binary = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um6uZK5EDzAm"
      },
      "source": [
        "#파라미터 설정\n",
        "vocab_size = 47136\n",
        "embedding_dim = 16\n",
        "max_length = 500\n",
        "padding_type='post'\n",
        "#oov_tok = \"<OOV>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvR9_VnTD8L9"
      },
      "source": [
        "#tokenizer에 fit\n",
        "tokenizer = Tokenizer(num_words = vocab_size)#, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FSCNBIYF-n7"
      },
      "source": [
        "# 사전 학습된 glove 불러오고 임베딩 층에 적용시키기 \n",
        "embedding_dict= dict()\n",
        "f = open('/content/drive/MyDrive/Novelist_Classification/embbeding/glove.txt', encoding='utf8')\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    temp = embedding_dict.get(word)\n",
        "    if temp is not None:\n",
        "        embedding_matrix[i] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXh_V23SGAY2"
      },
      "source": [
        "# Fasttext 임베딩 과정\n",
        "FT_embedding_matrix = np.zeros((vocab_size,100))\n",
        "\n",
        "def get_vector(word):\n",
        "    if word in FastText:\n",
        "        return FastText[word]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "for word, idx in word_index.items():\n",
        "    temp = get_vector(word)\n",
        "    if temp is not None:\n",
        "        FT_embedding_matrix[idx] = temp\n",
        "'''\n",
        "for idx, word in word_index.items():\n",
        "    if word in FastText:\n",
        "      embedding_vector = FastText[word]\n",
        "      FT_embedding_matrix[idx] = embedding_vector\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPB-18MKAFQ0"
      },
      "source": [
        "# Word2Vec 임베딩 과정\n",
        "W2V_embedding_matrix = np.zeros((vocab_size,300))\n",
        "\n",
        "def get_vector(word):\n",
        "    if word in Word2Vec_model:\n",
        "        return Word2Vec_model[word]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "for word,idx in word_index.items():\n",
        "    temp = get_vector(word)\n",
        "    if temp is not None:\n",
        "        W2V_embedding_matrix[idx] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPwyZt0EEHW"
      },
      "source": [
        "#데이터를 sequence로 변환해주고 padding 해줍니다.\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIaO8LHPU7Qh"
      },
      "source": [
        "# 가벼운 NLP 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2CxfUPZEOu0"
      },
      "source": [
        "#가벼운 NLP모델 생성\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxUTpZnPEXJX",
        "outputId": "e643e71e-5dd0-484e-84e6-7cd429f78470"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 500, 16)           754176    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_3 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 24)                408       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 125       \n",
            "=================================================================\n",
            "Total params: 754,709\n",
            "Trainable params: 754,709\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51YtSBBiEcMv",
        "outputId": "7cb54f4f-ed99-4874-bca6-ea1d7fa76bca"
      },
      "source": [
        "# fit model\n",
        "num_epochs = 20\n",
        "history = model.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, \n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1372/1372 - 17s - loss: 1.5650 - accuracy: 0.2754 - val_loss: 1.5452 - val_accuracy: 0.2695\n",
            "Epoch 2/20\n",
            "1372/1372 - 16s - loss: 1.4464 - accuracy: 0.3755 - val_loss: 1.3431 - val_accuracy: 0.3936\n",
            "Epoch 3/20\n",
            "1372/1372 - 15s - loss: 1.2297 - accuracy: 0.4932 - val_loss: 1.1785 - val_accuracy: 0.5155\n",
            "Epoch 4/20\n",
            "1372/1372 - 15s - loss: 1.1085 - accuracy: 0.5422 - val_loss: 1.1064 - val_accuracy: 0.5402\n",
            "Epoch 5/20\n",
            "1372/1372 - 15s - loss: 1.0281 - accuracy: 0.5877 - val_loss: 1.0500 - val_accuracy: 0.5712\n",
            "Epoch 6/20\n",
            "1372/1372 - 15s - loss: 0.9586 - accuracy: 0.6246 - val_loss: 1.0058 - val_accuracy: 0.6074\n",
            "Epoch 7/20\n",
            "1372/1372 - 15s - loss: 0.8961 - accuracy: 0.6558 - val_loss: 0.9543 - val_accuracy: 0.6311\n",
            "Epoch 8/20\n",
            "1372/1372 - 15s - loss: 0.8346 - accuracy: 0.6846 - val_loss: 0.9184 - val_accuracy: 0.6503\n",
            "Epoch 9/20\n",
            "1372/1372 - 15s - loss: 0.7783 - accuracy: 0.7087 - val_loss: 0.8838 - val_accuracy: 0.6663\n",
            "Epoch 10/20\n",
            "1372/1372 - 16s - loss: 0.7310 - accuracy: 0.7298 - val_loss: 0.8553 - val_accuracy: 0.6814\n",
            "Epoch 11/20\n",
            "1372/1372 - 16s - loss: 0.6866 - accuracy: 0.7504 - val_loss: 0.8329 - val_accuracy: 0.6900\n",
            "Epoch 12/20\n",
            "1372/1372 - 15s - loss: 0.6468 - accuracy: 0.7667 - val_loss: 0.8559 - val_accuracy: 0.6783\n",
            "Epoch 13/20\n",
            "1372/1372 - 16s - loss: 0.6112 - accuracy: 0.7803 - val_loss: 0.8046 - val_accuracy: 0.6989\n",
            "Epoch 14/20\n",
            "1372/1372 - 15s - loss: 0.5821 - accuracy: 0.7895 - val_loss: 0.8001 - val_accuracy: 0.7065\n",
            "Epoch 15/20\n",
            "1372/1372 - 15s - loss: 0.5583 - accuracy: 0.7979 - val_loss: 0.7904 - val_accuracy: 0.7095\n",
            "Epoch 16/20\n",
            "1372/1372 - 15s - loss: 0.5339 - accuracy: 0.8064 - val_loss: 0.7820 - val_accuracy: 0.7147\n",
            "Epoch 17/20\n",
            "1372/1372 - 15s - loss: 0.5131 - accuracy: 0.8149 - val_loss: 0.7834 - val_accuracy: 0.7142\n",
            "Epoch 18/20\n",
            "1372/1372 - 15s - loss: 0.4944 - accuracy: 0.8230 - val_loss: 0.7799 - val_accuracy: 0.7198\n",
            "Epoch 19/20\n",
            "1372/1372 - 15s - loss: 0.4777 - accuracy: 0.8291 - val_loss: 0.7821 - val_accuracy: 0.7210\n",
            "Epoch 20/20\n",
            "1372/1372 - 15s - loss: 0.4596 - accuracy: 0.8343 - val_loss: 0.7943 - val_accuracy: 0.7196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f_CziRQbntL"
      },
      "source": [
        "#가벼운 NLP모델 생성(Using FastText)\n",
        "model_using_FT = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 100, weights = [FT_embedding_matrix] ,input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iqVYWWCeZwB",
        "outputId": "ee002f2f-9fe9-4d92-dd89-3c84d982fa9d"
      },
      "source": [
        "# compile model\n",
        "model_using_FT.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_using_FT.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 100)          4713600   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 24)                2424      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 125       \n",
            "=================================================================\n",
            "Total params: 4,716,149\n",
            "Trainable params: 4,716,149\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CGSwTffeh8y",
        "outputId": "9507f7e3-653d-4f1a-8013-9e3874efd844"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=4, verbose=1)\n",
        "\n",
        "# fit model\n",
        "num_epochs = 20\n",
        "history = model_using_FT.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, \n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1372/1372 - 80s - loss: 1.5443 - accuracy: 0.3047 - val_loss: 1.4692 - val_accuracy: 0.4145\n",
            "Epoch 2/20\n",
            "1372/1372 - 78s - loss: 1.3030 - accuracy: 0.4614 - val_loss: 1.1872 - val_accuracy: 0.5059\n",
            "Epoch 3/20\n",
            "1372/1372 - 81s - loss: 1.0778 - accuracy: 0.5659 - val_loss: 1.0403 - val_accuracy: 0.5986\n",
            "Epoch 4/20\n",
            "1372/1372 - 80s - loss: 0.9152 - accuracy: 0.6514 - val_loss: 0.9070 - val_accuracy: 0.6514\n",
            "Epoch 5/20\n",
            "1372/1372 - 78s - loss: 0.7893 - accuracy: 0.7107 - val_loss: 0.8221 - val_accuracy: 0.7005\n",
            "Epoch 6/20\n",
            "1372/1372 - 78s - loss: 0.6962 - accuracy: 0.7496 - val_loss: 0.7973 - val_accuracy: 0.7026\n",
            "Epoch 7/20\n",
            "1372/1372 - 80s - loss: 0.6245 - accuracy: 0.7775 - val_loss: 0.7405 - val_accuracy: 0.7319\n",
            "Epoch 8/20\n",
            "1372/1372 - 80s - loss: 0.5763 - accuracy: 0.7914 - val_loss: 0.7196 - val_accuracy: 0.7351\n",
            "Epoch 9/20\n",
            "1372/1372 - 80s - loss: 0.5339 - accuracy: 0.8084 - val_loss: 0.7065 - val_accuracy: 0.7458\n",
            "Epoch 10/20\n",
            "1372/1372 - 79s - loss: 0.5032 - accuracy: 0.8181 - val_loss: 0.7359 - val_accuracy: 0.7215\n",
            "Epoch 11/20\n",
            "1372/1372 - 78s - loss: 0.4719 - accuracy: 0.8304 - val_loss: 0.7002 - val_accuracy: 0.7514\n",
            "Epoch 12/20\n",
            "1372/1372 - 80s - loss: 0.4474 - accuracy: 0.8394 - val_loss: 0.7018 - val_accuracy: 0.7485\n",
            "Epoch 13/20\n",
            "1372/1372 - 81s - loss: 0.4273 - accuracy: 0.8455 - val_loss: 0.7185 - val_accuracy: 0.7429\n",
            "Epoch 14/20\n",
            "1372/1372 - 81s - loss: 0.4075 - accuracy: 0.8544 - val_loss: 0.7354 - val_accuracy: 0.7370\n",
            "Epoch 15/20\n",
            "1372/1372 - 81s - loss: 0.3935 - accuracy: 0.8571 - val_loss: 0.7512 - val_accuracy: 0.7383\n",
            "Epoch 00015: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQbHQ1YxAfm7"
      },
      "source": [
        "#가벼운 NLP모델 생성(Using Word2Vec)\n",
        "model_using_W2V = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 300, weights = [W2V_embedding_matrix] ,input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBW5Xj5mAhCN",
        "outputId": "6eefe7f2-f877-4776-e03b-24d03fac0147"
      },
      "source": [
        "# compile model\n",
        "model_using_W2V.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_using_W2V.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 300)          14140800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 24)                7224      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 125       \n",
            "=================================================================\n",
            "Total params: 14,148,149\n",
            "Trainable params: 14,148,149\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaOGlr3SAjKY",
        "outputId": "9aa4907b-84d8-40f1-a00f-45590d372f76"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=4, verbose=1)\n",
        "\n",
        "# fit model\n",
        "num_epochs = 20\n",
        "history = model_using_W2V.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, \n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1372/1372 - 274s - loss: 1.4404 - accuracy: 0.3739 - val_loss: 1.1875 - val_accuracy: 0.5454\n",
            "Epoch 2/20\n",
            "1372/1372 - 272s - loss: 1.0449 - accuracy: 0.5868 - val_loss: 0.9771 - val_accuracy: 0.6449\n",
            "Epoch 3/20\n",
            "1372/1372 - 255s - loss: 0.8325 - accuracy: 0.6896 - val_loss: 0.8175 - val_accuracy: 0.6887\n",
            "Epoch 4/20\n",
            "1372/1372 - 258s - loss: 0.6901 - accuracy: 0.7499 - val_loss: 0.7454 - val_accuracy: 0.7265\n",
            "Epoch 5/20\n",
            "1372/1372 - 256s - loss: 0.6018 - accuracy: 0.7821 - val_loss: 0.7372 - val_accuracy: 0.7239\n",
            "Epoch 6/20\n",
            "1372/1372 - 255s - loss: 0.5423 - accuracy: 0.8024 - val_loss: 0.7250 - val_accuracy: 0.7337\n",
            "Epoch 7/20\n",
            "1372/1372 - 248s - loss: 0.4957 - accuracy: 0.8193 - val_loss: 0.7388 - val_accuracy: 0.7259\n",
            "Epoch 8/20\n",
            "1372/1372 - 245s - loss: 0.4594 - accuracy: 0.8309 - val_loss: 0.7434 - val_accuracy: 0.7302\n",
            "Epoch 9/20\n",
            "1372/1372 - 238s - loss: 0.4287 - accuracy: 0.8422 - val_loss: 0.7582 - val_accuracy: 0.7268\n",
            "Epoch 10/20\n",
            "1372/1372 - 256s - loss: 0.4028 - accuracy: 0.8531 - val_loss: 0.7596 - val_accuracy: 0.7366\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jXKoOWIC6g0"
      },
      "source": [
        "# predict values\n",
        "pred = model_using_FT.predict(test_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4S8eUx5FDFO"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "tZhbUjhXE3Yr",
        "outputId": "daff6f37-fbb1-408a-8232-69889a48ccde"
      },
      "source": [
        "# submission\n",
        "sample_submission[['0','1','2','3','4','5','6','7']] = pred\n",
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19612</th>\n",
              "      <td>19612</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19613</th>\n",
              "      <td>19613</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19614</th>\n",
              "      <td>19614</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19615</th>\n",
              "      <td>19615</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19616</th>\n",
              "      <td>19616</td>\n",
              "      <td>0.244451</td>\n",
              "      <td>0.130629</td>\n",
              "      <td>0.211542</td>\n",
              "      <td>0.27031</td>\n",
              "      <td>0.143069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19617 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index         0         1         2        3         4\n",
              "0          0  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "1          1  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "2          2  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "3          3  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "4          4  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "...      ...       ...       ...       ...      ...       ...\n",
              "19612  19612  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "19613  19613  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "19614  19614  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "19615  19615  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "19616  19616  0.244451  0.130629  0.211542  0.27031  0.143069\n",
              "\n",
              "[19617 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu8Wb207FP-D"
      },
      "source": [
        "sample_submission.to_csv('/content/drive/MyDrive/Novelist_Classification/datasets/submission.csv', index = False, encoding = 'utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWbUUwqKBdie"
      },
      "source": [
        "#Result of Train  \n",
        "## Base stopwords + Keras Embedding  \n",
        "1372/1372 - 7s - loss: 0.4276 - accuracy: 0.8455 - val_loss: 0.7625 - val_accuracy: 0.7305\n",
        "## Base stopwords + FastText  \n",
        "1372/1372 - 78s - loss: 0.4719 - accuracy: 0.8304 - val_loss: 0.7002 - val_accuracy: 0.7514\n",
        "## Base stopwords + Word2Vec\n",
        "1372/1372 - 256s - loss: 0.4028 - accuracy: 0.8531 - val_loss: 0.7596 - val_accuracy: 0.7366\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsL4zuM7k3FN"
      },
      "source": [
        "# LSTM\n",
        "- 참고 논문 1 : [A Text Sentiment Classification Method Based on LSTM-CNN ](https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002537588)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMVIetvuUgUs"
      },
      "source": [
        "### LSTM + FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cS6IUJeUgUt"
      },
      "source": [
        "#### unit 30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toAvB1V4k5YE"
      },
      "source": [
        "model_FT_lstm_30 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 100,weights = [FT_embedding_matrix] ,input_length=max_length),\n",
        "    tf.keras.layers.LSTM(units=30),\n",
        "    \n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model_FT_lstm_30.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_FT_lstm_30.summary())\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, \n",
        "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "# fit model\n",
        "num_epochs = 20\n",
        "history_FT_30 = model_FT_lstm_30.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, batch_size = 256,\n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSwXWxtoUgUt"
      },
      "source": [
        "#### unit 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gaGtXbzlSRa"
      },
      "source": [
        "model_FT_lstm_50 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 100,weights = [FT_embedding_matrix] ,input_length=max_length),\n",
        "    tf.keras.layers.LSTM(units=50),\n",
        "    \n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model_FT_lstm_50.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_FT_lstm_50.summary())\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, \n",
        "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "# fit model\n",
        "num_epochs = 20\n",
        "history_FT_50 = model_FT_lstm_50.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, batch_size = 256,\n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTEjtw6GUgUu"
      },
      "source": [
        "#### unit 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyR90vFtUgUu"
      },
      "source": [
        "model_FT_lstm_128 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 100,weights = [FT_embedding_matrix] ,input_length=max_length),\n",
        "    tf.keras.layers.LSTM(units=128),\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model_FT_lstm_128.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_FT_lstm_128.summary())\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, \n",
        "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "# fit model\n",
        "num_epochs = 1000\n",
        "history_FT_128 = model_FT_lstm_128.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, batch_size = 256,\n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQBjccX4UgUv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_model_perfomance(history,name):\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    plt.figure(1)\n",
        "    plt.plot(history.history['loss'], lw=2.0, color='b', label='train')\n",
        "    plt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.plot(history.history['accuracy'], lw=2.0, color='b', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], lw=2.0, color='r', label='val')\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "plot_model_perfomance(history_FT_128,'FastText + LSTM(128)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U239GipdUgUv"
      },
      "source": [
        "#### LSTM-CNN 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt0sLT4bUgUw"
      },
      "source": [
        "model_FT_lstm_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 100,weights = [FT_embedding_matrix] ,input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
        "    tf.keras.layers.LSTM(units=128),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# compile model\n",
        "model_FT_lstm_cnn.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_FT_lstm_cnn.summary())\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=7 ,\n",
        "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "# fit model\n",
        "num_epochs = 100\n",
        "history_FT_lstm_cnn = model_FT_lstm_cnn.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, batch_size = 64,\n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xVoUKiVUgUx"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_model_perfomance(history,name):\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    plt.figure(1)\n",
        "    plt.plot(history.history['loss'], lw=2.0, color='b', label='train')\n",
        "    plt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.figure(2)\n",
        "    plt.plot(history.history['accuracy'], lw=2.0, color='b', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], lw=2.0, color='r', label='val')\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "plot_model_perfomance(history_FT_lstm_cnn,'FastText + LSTM-CNN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpkCeL57UgUy"
      },
      "source": [
        "pred_FT_128 = model_FT_lstm_128.predict(test_padded)\n",
        "sample_submission[['0','1','2','3','4','5','6','7']] = pred_FT_128\n",
        "sample_submission.to_csv('submission_FT_128.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzuT0YLmUgUy"
      },
      "source": [
        "pred_FT_cnn = model_FT_lstm_cnn.predict(test_padded)\n",
        "sample_submission[['0','1','2','3','4','5','6','7']] = pred_FT_cnn\n",
        "sample_submission.to_csv('submission_FT_cnn.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ifF03c2UgUy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvHtWL0kUgUy"
      },
      "source": [
        "### LSTM + Glove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZcRxaDwUgUz"
      },
      "source": [
        "#### unit 30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ezxmH1sUgUz"
      },
      "source": [
        "model_G_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, 50,weights = [glove_embedding_matrix] ,input_length=max_length),\n",
        "\n",
        "    tf.keras.layers.LSTM(units=30),\n",
        "\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model_G_lstm.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_G_lstm.summary())\n",
        "\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, \n",
        "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "# fit model\n",
        "num_epochs = 30\n",
        "history_G_30 = model_G_lstm.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, batch_size = 256,\n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmEfSY3cUgUz"
      },
      "source": [
        "#### unit 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prpJqy8_UgUz"
      },
      "source": [
        "model_G_lstm_50 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, 50,weights = [glove_embedding_matrix] ,input_length=max_length),\n",
        "\n",
        "    tf.keras.layers.LSTM(units=50),\n",
        "\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model_G_lstm_50.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_G_lstm_50.summary())\n",
        "\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5,\n",
        "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "\n",
        "# fit model\n",
        "num_epochs = 20\n",
        "history_G_50 = model_G_lstm_50.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2,batch_size = 256,\n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSaDfuZ0UgUz"
      },
      "source": [
        "#### unit 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zzgid6HUgU0"
      },
      "source": [
        "model_G_lstm_128 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, 50,weights = [glove_embedding_matrix] ,input_length=max_length),\n",
        "    tf.keras.layers.LSTM(units=128),\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "# compile model\n",
        "model_G_lstm_128.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_G_lstm_128.summary())\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, \n",
        "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "# fit model\n",
        "num_epochs = 1000\n",
        "history_G_128 = model_G_lstm_128.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, batch_size = 256,\n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiofEanxUgU0"
      },
      "source": [
        "#에포크에 따른 loss와 accuracy 변화 그래프\n",
        "def plot_model_perfomance(history,name):\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    plt.figure(1)\n",
        "\n",
        "    plt.plot(history.history['loss'], lw=2.0, color='b', label='train')\n",
        "    plt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.figure(2)\n",
        "\n",
        "    plt.plot(history.history['accuracy'], lw=2.0, color='b', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], lw=2.0, color='r', label='val')\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "plot_model_perfomance(history_G_128,'Glove + LSTM(128)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avouwug3UgU0"
      },
      "source": [
        "#### LSTM-CNN 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tHkarRwUgU0"
      },
      "source": [
        "model_G_lstm_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, 50,weights = [glove_embedding_matrix] ,input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
        "    tf.keras.layers.LSTM(units=128),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# compile model\n",
        "model_G_lstm_cnn.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model summary\n",
        "print(model_G_lstm_cnn.summary())\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=7 ,\n",
        "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "# fit model\n",
        "num_epochs = 100\n",
        "history_G_lstm_cnn = model_G_lstm_cnn.fit(train_padded, y_train, \n",
        "                    epochs=num_epochs, verbose=2, batch_size = 64,\n",
        "                    validation_split=0.2, callbacks = [early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRAsMM-EUgU0"
      },
      "source": [
        "def plot_model_perfomance(history,name):\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    plt.figure(1)\n",
        "\n",
        "    plt.plot(history.history['loss'], lw=2.0, color='b', label='train')\n",
        "    plt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.figure(2)\n",
        "\n",
        "    plt.plot(history.history['accuracy'], lw=2.0, color='b', label='train')\n",
        "    plt.plot(history.history['val_accuracy'], lw=2.0, color='r', label='val')\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "plot_model_perfomance(history_G_lstm_cnn,'Glove + LSTM-CNN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFkR67oBUgU1"
      },
      "source": [
        "pred_G_128 = model_G_lstm_128.predict(test_padded)\n",
        "sample_submission[['0','1','2','3','4','5','6','7']] = pred_G_128\n",
        "sample_submission.to_csv('submission_G_128.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYtHJBOtUgU1"
      },
      "source": [
        "pred_G_cnn = model_G_lstm_cnn.predict(test_padded)\n",
        "sample_submission[['0','1','2','3','4','5','6','7']] = pred_G_cnn\n",
        "sample_submission.to_csv('submission_G_cnn.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}